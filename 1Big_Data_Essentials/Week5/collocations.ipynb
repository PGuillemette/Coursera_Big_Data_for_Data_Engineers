from pyspark import SparkConf, SparkContext
from math import log
import re

# Var & functions

# Get stop_words
with open('/datasets/stop_words_en.txt', 'r') as f:
    stop_words = set(f.read().split())

# Parse article return an array of words
def parse_article(line):
    try:
        article_id, text = str(line.rstrip()).split('\t', 1)
        text = re.sub("^\W+|\W+$", "", text)
        words = re.split("\W*\s+\W*", text)
        return words
    except ValueError as e:
        return []


def create_bigrams(words):
    bigrams = []
    for i, word in enumerate(words[:-1]):
        pair = u'_'.join((word, words[i+1])).encode('utf-8')
        bigrams.append((pair, 1))
    return bigrams

def calc_npmi(pair, cnt, words_occurrences_dict, total_num_of_words, total_num_of_pairs):
    word1, word2 = pair.split('_')
    p_a = words_occurrences_dict[word1] / total_num_of_words
    p_b = words_occurrences_dict[word2] / total_num_of_words
    
    pmi_ab = cnt / total_num_of_pairs
    pmi_a_b = log(pmi_ab / (p_a * p_b))
    
    nmpi_a_b = pmi_a_b / -log(pmi_ab)
    return (pair, nmpi_a_b)

# Context
sc = SparkContext(conf=SparkConf().setAppName('myApp').setMaster('local[*]'))

# Prepare data
wiki = sc.textFile("/data/wiki/en_articles_part/articles-part", 16).map(parse_article)

# Manage data

## lowercase all words
wiki_lower = wiki.map(lambda words: [x.lower() for x in words])

## words not in stop_words_en.txt
wiki_filt = wiki_lower.map(lambda words: [x for x in words if x not in stop_words])

## create bigrams
wiki_bigrams = wiki_filt.flatMap(create_bigrams)

## aggregate counters
wiki_red = wiki_bigrams.reduceByKey(lambda a, b: a + b)

## filter values by counter
wiki_red_filt = wiki_red.filter(lambda pair, cnt: cnt >= 500)



## total number of words
tot_num_words = wiki_filt.map(lambda words: len(words))
tot_num_words = tot_num_words.reduce(lambda a, b: a + b)

## total number of words pairs
tot_num_pairs = wiki_filt.map(lambda words: len(words) - 1)
tot_num_pairs = tot_num_pairs.reduce(lambda a, b: a + b)

## number of each word occurrences
words_occ = wiki_filt.flatMap(lambda words: [(x, 1) for x in words])
words_occ = words_occ.reduceByKey(lambda a, b: a + b)
words_occ = words_occ.filter(lambda pair, cnt: cnt >= 500)
words_occ = words_occ.collect()

words_occ_dict = dict()
for item, cnt in words_occ:
    words_occ_dict[item] = cnt

pairs_npmi = wiki_red_filt\
    .map(lambda pair, cnt: calc_npmi(pair, cnt, words_occ_dict, tot_num_words, tot_num_pairs))\
    .map(lambda a, b: (b, a))\
    .sortByKey(False)\
    .map(lambda a, b: (b, a))\
    .take(39)

for pair, npmi in pairs_npmi:
    print(pair)
